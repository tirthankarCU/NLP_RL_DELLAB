{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4590,"status":"ok","timestamp":1676311539564,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":420},"id":"keLU9H2OZotC","outputId":"d29e77bc-1aa1-403c-9d2e-2dcddf44bde2"},"outputs":[],"source":["from google.colab import drive\n","from os.path import join  \n","\n","def mount_drive(ROOT):\n","    drive.mount(ROOT, force_remount=True)\n","\n","ROOT = '/content/drive'\n","mount_drive(ROOT)\n","\n","MY_GOOGLE_DRIVE_PATH = 'MyDrive/RL_NLP/' \n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","%cd \"{PROJECT_PATH}\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26304,"status":"ok","timestamp":1676311569595,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":420},"id":"sw5HrcjQYRAc","outputId":"da9fc892-b7ea-4a3b-a464-19ed8f1a03d6"},"outputs":[],"source":["!pip3 install virtualenv\n","!virtualenv -p python3.8.10 python38\n","\n","!source /content/drive/MyDrive/Colab/RL_NLP/python38/bin/activate\n","!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10002,"status":"ok","timestamp":1676311585763,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":420},"id":"0MNuM_mfchFj","outputId":"f8c186ba-d15e-4394-e023-1eee44306ceb"},"outputs":[],"source":["!pip install -r \"{PROJECT_PATH}/metadata/requirements.txt\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## First time only run.\n","\n","!git clone https://github.com/Farama-Foundation/gym-examples\n","\n","%cd gym-examples\n","\n","!pip install -e .\n","\n","%cd .."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# RUN FROM HERE."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1254,"status":"ok","timestamp":1676311593426,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":420},"id":"xG2FfgrYeLGS"},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import time\n","import utils as U\n","import model as M\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt \n","import json\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -e ."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gym\n","%cd gym-examples\n","import gym_examples\n","%cd ..\n","import numpy as np\n","\n","episodes=2\n","env = gym.make('gym_examples/RlNlpWorld-v0',render_mode=\"rgb_array\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Random Policy."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def policy(observation):\n","    return np.random.randint(0,6)\n","\n","dbg=True\n","for _ in range(episodes):\n","    cumulative_reward,steps=0,0\n","    observation, info = env.reset(seed=42)\n","    cnt=0\n","    while True:\n","        action = policy(observation)  # User-defined policy function\n","        observation, reward, terminated, truncated, info = env.step(action)\n","        cumulative_reward+=reward\n","        steps+=1\n","        time.sleep(0.5)\n","        if dbg==True:\n","            print(f'cumulative_reward {cumulative_reward}; action {action}')\n","            print(observation[\"visual\"].shape)\n","            output=U.crop_resize(observation[\"visual\"])\n","            print(output.shape)\n","            plt.imshow(output)\n","            plt.savefig('SuperReduced.png')\n","            output_=np.mean(output,axis=2)\n","            plt.imshow(output_)\n","            plt.savefig('BW.png')\n","            print(output_.shape)\n","            break\n","        if terminated or truncated:\n","            break\n","    print(f'Cumulative Reward ~ {cumulative_reward}; TimeTaken ~ {steps}')\n","env.close()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q-Learning w/o NLP. (NAIVE MODEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_actions=6\n","update_after_actions=2**8\n","update_target_network=2**13\n","frame_cnt,epsilon_greedy=0,100\n","episodes,mxSteps=2,50\n","episode_reward_hist,episode_avg_reward=[],[]\n","epsilon,epsilon_min,epsilon_max=1.0,0.005,1.0 \n","epsilon_interval = (\n","    epsilon_max - epsilon_min\n",") \n","epsilon_greedy_frames = 100000.0\n","batch_size=2**10\n","action=-1\n","replay_mem={'state':[],'next_state':[],'action':[],'reward':[],'small_n_state':[]}\n","mx_replay_memory=100000\n","gamma=0.9\n","\n","# MODEL\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model=M.NNModel().to(device)\n","model_traget=M.NNModel().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n","epochs=20\n","\n","for _ in range(episodes):\n","    episode_reward=0\n","    state, info = env.reset(seed=42)\n","    for __ in range(mxSteps):\n","        frame_cnt+=1\n","        epsilon -= epsilon_interval / epsilon_greedy_frames\n","        epsilon = max(epsilon, epsilon_min)\n","        if frame_cnt<=epsilon_greedy or epsilon>np.random.rand(1)[0]:\n","            action=np.random.choice(num_actions)\n","            # with open('my_dict.json', 'w') as f:\n","            #     U.phi(state)\n","            #     state[\"visual\"]=state[\"visual\"].tolist()\n","            #     json.dump(state,f)\n","        else:\n","            action=(M.predict(model,[state],device))[1]\n","        next_state,reward,terminated,truncated,info=env.step(action)\n","        episode_reward+=reward \n","        temp_next_state=copy.deepcopy(next_state) #To Save Small Image.\n","        U.phi(state);replay_mem['state'].append(state)\n","        U.phi(next_state);replay_mem['next_state'].append(next_state)\n","        U.phiXtra(temp_next_state);replay_mem['small_n_state'].append(temp_next_state)\n","        replay_mem['action'].append(U.oneHot(num_actions,action))\n","        replay_mem['reward'].append(reward)\n","        state=next_state \n","        if True or frame_cnt%update_after_actions==0 and len(replay_mem['reward'])>batch_size:\n","            choices=np.random.choice( range( len(replay_mem['reward']) ), size=batch_size )\n","            STATE=[replay_mem['state'][id] for id in choices]\n","            NEXT_STATE=[replay_mem['next_state'][id] for id in choices]\n","            SMALL_NEXT_STATE=[replay_mem['small_n_state'][id] for id in choices]\n","            ACTION=np.array([replay_mem['action'][id] for id in choices])\n","            REWARD=np.array([replay_mem['reward'][id] for id in choices])\n","            IMG=np.array([_state_[\"visual\"] for _state_ in STATE])\n","            reward_true=REWARD+gamma*(M.predict(model_traget,NEXT_STATE,device))[0]\n","            continue\n","            for epoch in range(epochs):\n","                M.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,epoch)\n","        if frame_cnt%update_target_network==0:\n","            model_traget.load_state_dict(model.state_dict())\n","        if len(replay_mem['reward'])>mx_replay_memory:\n","            del replay_mem['state'][:1]\n","            del replay_mem['next_state'][:1]\n","            del replay_mem['action'][:1]\n","            del replay_mem['reward'][:1]\n","        if terminated or truncated:\n","            break\n","    \n","    episode_reward_hist.append(episode_reward)\n","    episode_avg_reward.append(episode_reward/__)\n","\n","# Save Relevant Results\n","np.save('results/ep_tot.npy',np.array(episode_reward_hist))\n","np.save('results/ep_avg.npy',np.array(episode_avg_reward))\n","torch.save(model.state_dict(),'results/model.ml')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading Results.\n","episode_reward_hist=np.load('results/ep_tot.npy')\n","episode_avg_reward=np.load('results/ep_avg.npy')\n","U.plot(episode_reward_hist,'Total Reward','Total Reward V/S Episodes')\n","U.plot(episode_avg_reward,'Avg. Reward','Avg. Reward Per Step V/S Episodes')\n","model_tr=M.NNModel().to(device)\n","model_tr.load_state_dict(torch.load('results/model.ml'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(replay_mem)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(replay_mem[\"state\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(replay_mem[\"action\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### TBD\n","\n","~~1. image training reduce.~~\n","\n","~~2. save model_weights in File.~~\n","\n","~~3. save replay memory in File.~~[Will do if necessary]\n","\n","4. check whether for all weights -> requries_grad=True\n","5. upload the code in google collab and make it train.\n","6. write instruction auto gen code. "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN2APZKWvq8FDNl0p6y/GRU","provenance":[]},"kernelspec":{"display_name":"tf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"607977446fd7db0f3e2ddba643f6a09248589e32f4e48c8af2325eb4debcc38d"}}},"nbformat":4,"nbformat_minor":0}
